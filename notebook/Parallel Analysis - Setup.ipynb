{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><head>\n",
    "\n",
    "\n",
    "<!-- Load require.js. Delete this if your page already loads require.js -->\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js\" integrity=\"sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=\" crossorigin=\"anonymous\"></script>\n",
    "<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n",
    "<script type=\"application/vnd.jupyter.widget-state+json\">\n",
    "{\n",
    "    \"version_major\": 2,\n",
    "    \"version_minor\": 0,\n",
    "    \"state\": {}\n",
    "}\n",
    "</script>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "# Parallel Analysis - Setup and Run\n",
    "\n",
    "This notebook sets up and runs a parallel analysis on AWS.\n",
    "\n",
    "You can learn more at [RGLab/scamp](https://github.com/RGLab/scamp)\n",
    "\n",
    "The steps are:\n",
    "\n",
    "* Create the code\n",
    "* Configure AWS connection\n",
    "* Configure the data you want to process\n",
    "* Upload the data to AWS (to S3)\n",
    "* Configure the processing (code to run, CPU and memory and storage to use)\n",
    "* Kick off the processing\n",
    "\n",
    "\n",
    "### Step A - Create the Code\n",
    "\n",
    "1. Import the necessary libraries\n",
    "2. Create the functions we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import boto3\n",
    "import os.path\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/1392413/calculating-a-directorys-size-using-python \n",
    "def get_folder_size(local_directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(local_directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / 1000000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpu_needed (analysis_size_mb):\n",
    "    cpu_count = 4\n",
    "    if analysis_size_mb > 100:\n",
    "        cpu_count = 8\n",
    "    elif analysis_size_mb > 500:\n",
    "        cpu_count = 16\n",
    "    elif analysis_size_mb > 1000:\n",
    "        cpu_count = 32\n",
    "    return cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_needed (analysis_size_mb, cpu_count):\n",
    "    return 2.0 * cpu_count / 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories_and_sizes(parent_directory):\n",
    "    dir_sizes = {}\n",
    "    for analysis_dir in os.listdir(parent_directory):\n",
    "        analysis_dir_path = os.path.join(parent_directory, analysis_dir)\n",
    "        if os.path.isdir(analysis_dir_path):\n",
    "            dir_sizes[analysis_dir_path] = get_folder_size(analysis_dir_path)\n",
    "    return dir_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://gist.github.com/feelinc/d1f541af4f31d09a2ec3\n",
    "def upload_analysis(input_dir, s3_bucket, s3_prefix, client):\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            # construct the full local path\n",
    "            local_path = os.path.join(root, filename)\n",
    "\n",
    "            # construct the full S3 path\n",
    "            relative_path = os.path.relpath(local_path, input_dir)\n",
    "            s3_path = os.path.join(s3_prefix, relative_path)\n",
    "\n",
    "            # relative_path = os.path.relpath(os.path.join(root, filename))\n",
    "\n",
    "            print ('Searching \"%s\" in \"%s\"' % (s3_path, bucket))\n",
    "            try:\n",
    "                client.head_object(Bucket=s3_bucket, Key=s3_path)\n",
    "                print (\"Path found on S3! Skipping %s...\" % s3_path)\n",
    "\n",
    "                # try:\n",
    "                    # client.delete_object(Bucket=bucket, Key=s3_path)\n",
    "                # except:\n",
    "                    # print \"Unable to delete %s...\" % s3_path\n",
    "            except:\n",
    "                print (\"Uploading %s...\" % s3_path)\n",
    "                client.meta.client.upload_file(local_path, bucket, s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step B - Configure AWS connection\n",
    "\n",
    "1. Configure AWS account\n",
    "2. Configure AWS credentials (e.g. access key and secret key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'hackathon' profile\n"
     ]
    }
   ],
   "source": [
    "profile_name = 'hackathon'\n",
    "print(\"Using '{}' profile\".format(profile_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_session = boto3.session.Session(profile_name=profile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C - Configure the data you want to process\n",
    "\n",
    "1. Set the input directory\n",
    "2. See the list of analysis, confirm that's corrext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER THE NAME OF THE PARENT DIRECTORY\n",
    "parent_directory = '/Users/dnambi/Downloads/hackathon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = get_directories_and_sizes(parent_directory)\n",
    "#print(\"The analyses to run are {}\".format(list(analysis_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step D - Upload the data to AWS (to S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER THE NAME OF THE S3 BUCKET\n",
    "s3_bucket = 'fh-hdc-cytometry-hackathon'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "s3_client = cortex_session.resource('s3')\n",
    "prefix = 'input'\n",
    "upload_analysis(input_dir=parent_directory, s3_bucket=s3_bucket, s3_prefix=prefix, client=s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step E - Configure the processing\n",
    "\n",
    "1. Set the code location (GitHub repo)\n",
    "2. Set the startup command\n",
    "3. Configure CPU and memory for each analysis\n",
    "4. Configure storage for each analysis\n",
    "\n",
    "#### Steps E1 and E2 - set GitHub repo and analysis command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step E3 - Configure CPU and memory for each analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info for analysis\n",
    "analysis_info = {}\n",
    "for analysis_dir in os.listdir(parent_directory):\n",
    "    analysis_dir_path = os.path.join(parent_directory, analysis_dir)\n",
    "    if os.path.isdir(analysis_dir_path):\n",
    "        folder_size_mb = get_folder_size(analysis_dir_path)\n",
    "        cpu_count_needed = get_cpu_needed(folder_size_mb)\n",
    "        mem_gb_needed = get_memory_needed(folder_size_mb, cpu_count_needed)\n",
    "        #print (\"Analysis {} is {} MB and needs {} CPU and {} GB of RAM\".format(analysis_dir_path,folder_size_mb, cpu_count_needed, mem_gb_needed))\n",
    "        analysis_params = {\"size_mb\": folder_size_mb, \"cpu_count\": cpu_count_needed, \"mem_gb\": mem_gb_needed}\n",
    "        analysis_info[analysis_dir_path] = analysis_params\n",
    "#print (analysis_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_params = {} \n",
    "# key is the analysis folder name\n",
    "# value is a dict of input size, CPU info, memory info, storage info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step F - Kick off the processing\n",
    "\n",
    "1. Name the batch\n",
    "   * Add the ability to email somene when it's done?\n",
    "2. Kick off the analysis (start the batch)\n",
    "3. Confirm it has started correctly\n",
    "\n",
    "#### Step F1 - Name the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex = boto3.session.Session(profile_name='hackathon')\n",
    "batch_client = cortex.client('batch', region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_job(r_script_s3_url\n",
    "                     , s3_input_dir\n",
    "                     , s3_output_dir\n",
    "                     , cpu_count\n",
    "                     , mem_mb\n",
    "                     , job_name\n",
    "                     , job_queue\n",
    "                     , job_definition\n",
    "                     , client\n",
    "                     , batch_file_s3_url = 's3://fh-hdc-cytometry-hackathon/startup.sh'):\n",
    "    response = client.submit_job(\n",
    "        jobName = job_name\n",
    "        ,jobQueue = job_queue\n",
    "        ,jobDefinition=job_definition\n",
    "        ,containerOverrides={\n",
    "            'vcpus': cpu_count,\n",
    "            'memory': mem_mb,\n",
    "            'command': [\n",
    "                'startup.sh',\n",
    "            ],\n",
    "            \"environment\": [ \n",
    "             { \n",
    "                \"name\": \"BATCH_FILE_TYPE\",\n",
    "                \"value\": \"script\"\n",
    "             },\n",
    "             { \n",
    "                \"name\": \"BATCH_FILE_S3_URL\",\n",
    "                \"value\": batch_file_s3_url\n",
    "             },\n",
    "             { \n",
    "                \"name\": \"R_SCRIPT_S3_URL\",\n",
    "                \"value\": r_script_s3_url\n",
    "             },\n",
    "             { \n",
    "                \"name\": \"S3_INPUT_DIR\",\n",
    "                \"value\": s3_input_dir\n",
    "             },\n",
    "             { \n",
    "                \"name\": \"S3_OUTPUT_DIR\",\n",
    "                \"value\": s3_output_dir\n",
    "             }\n",
    "            ]\n",
    "        }\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "job_queue = 'dnambi-hackathon-queue'\n",
    "job_definition = 'fetch_and_run_1:5'\n",
    "job_name = 'hackathon-cytometry-testjob-002fcs'\n",
    "r_script_s3_url = 's3://fh-hdc-cytometry-hackathon/r-code/processData.R'\n",
    "s3_input_dir = 's3://fh-hdc-cytometry-hackathon/input/002.fcs/'\n",
    "s3_output_dir = 's3://fh-hdc-cytometry-hackathon/input/002.fcs/'\n",
    "cpu_count = 16\n",
    "mem_mb = 31000\n",
    "\n",
    "\n",
    "response = create_batch_job (r_script_s3_url = r_script_s3_url\n",
    "                     , s3_input_dir = s3_input_dir\n",
    "                     , s3_output_dir = s3_output_dir\n",
    "                     , cpu_count = cpu_count\n",
    "                     , mem_mb = mem_mb\n",
    "                     , job_name = job_name\n",
    "                     , job_queue = job_queue\n",
    "                     , job_definition = job_definition\n",
    "                     , client = batch_client)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_queue = 'dnambi-hackathon-queue'\n",
    "job_definition = 'fetch_and_run_1:5'\n",
    "r_script_s3_url = 's3://fh-hdc-cytometry-hackathon/r-code/processDatav2.R'\n",
    "s3_input_base_dir = 's3://fh-hdc-cytometry-hackathon/input/'\n",
    "s3_output_base_dir = 's3://fh-hdc-cytometry-hackathon/input/'\n",
    "cpu_count = 12\n",
    "mem_mb = 23500\n",
    "\n",
    "for analysis_dir in os.listdir(parent_directory):\n",
    "    job_name = \"hackathon-cytometry-dnambi-tuningv2-\" + analysis_dir\n",
    "    job_name = str.replace(job_name,\".\",\"\")\n",
    "    s3_input_dir = s3_input_base_dir + analysis_dir + \"/\"\n",
    "    s3_output_dir = s3_output_base_dir + analysis_dir + \"/\"\n",
    "    response = create_batch_job (r_script_s3_url = r_script_s3_url\n",
    "                     , s3_input_dir = s3_input_dir\n",
    "                     , s3_output_dir = s3_output_dir\n",
    "                     , cpu_count = cpu_count\n",
    "                     , mem_mb = mem_mb\n",
    "                     , job_name = job_name\n",
    "                     , job_queue = job_queue\n",
    "                     , job_definition = job_definition\n",
    "                     , client = batch_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step F2 - Kick off the analysis processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step F3 - Confirm the processing has started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
